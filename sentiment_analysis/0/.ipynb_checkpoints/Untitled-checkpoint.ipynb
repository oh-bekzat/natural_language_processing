{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c303685-d208-45b5-8499-eb218399681e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import stanza\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.model_selection as skm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, ConfusionMatrixDisplay)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5267a206-7c9d-4238-b051-96aa01481ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('yelp_labelled.txt', header = None, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a27584f7-ed71-41ff-8076-d6516e24b512",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[0]\n",
    "y = data[1]\n",
    "x_train_t, x_temp, y_train, y_temp = skm.train_test_split(x, y, test_size=0.3, stratify=y, random_state=0)\n",
    "x_val_t, x_test_t, y_val, y_test = skm.train_test_split(x_temp, y_temp, test_size=2/3, stratify=y_temp, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2816e56-6101-42ea-94f5-3d1bc63d6750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation set accuracy: 0.85\n",
      "best configuration: {'min_df': 2, 'stop_words': None, 'binary': False, 'lowercase': True, 'alpha': 1.0}\n",
      "test set accuracy on best configuration: 0.82\n"
     ]
    }
   ],
   "source": [
    "min_dfs = [1, 2, 3]\n",
    "stop_word_options = [None, \"english\"]\n",
    "binary_options = [True, False]\n",
    "lowercase_options = [True, False]\n",
    "alphas = [0.01, 0.1, 1.0]\n",
    "\n",
    "best_acc = 0\n",
    "best_config = {}\n",
    "\n",
    "for min_df in min_dfs:\n",
    "    for stop_words in stop_word_options:\n",
    "        for binary in binary_options:\n",
    "            for lowercase in lowercase_options:\n",
    "                vectorizer = CountVectorizer(min_df=min_df, stop_words=stop_words, binary=binary, lowercase=lowercase)\n",
    "                x_train = vectorizer.fit_transform(x_train_t)\n",
    "                x_val = vectorizer.transform(x_val_t)\n",
    "                for alpha in alphas:\n",
    "                    clf = MultinomialNB(alpha=alpha)\n",
    "                    clf.fit(x_train, y_train)\n",
    "                    val_pred = clf.predict(x_val)\n",
    "                    acc = accuracy_score(y_val, val_pred)\n",
    "                    \n",
    "                    if acc > best_acc:\n",
    "                        best_acc = acc\n",
    "                        best_config = { 'min_df': min_df,\n",
    "                                        'stop_words': stop_words,\n",
    "                                        'binary': binary,\n",
    "                                        'lowercase': lowercase,\n",
    "                                        'alpha': alpha }\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=best_config['min_df'], stop_words=best_config['stop_words'], binary=best_config['binary'], lowercase=best_config['lowercase'])\n",
    "x_train = vectorizer.fit_transform(x_train_t)\n",
    "x_test = vectorizer.transform(x_test_t)\n",
    "\n",
    "clf = MultinomialNB(alpha=best_config['alpha'])\n",
    "clf.fit(x_train, y_train)\n",
    "test_pred = clf.predict(x_test)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "\n",
    "print(f'best validation set accuracy: {best_acc}')\n",
    "print(f'best configuration: {best_config}')\n",
    "print(f'test set accuracy on best configuration: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e7e2a67-aa16-44c9-9e82-afe4f98d5235",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiwords = {}\n",
    "with open('SentiWords_1.1.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        parts = line.strip().split('\\t')\n",
    "        key, value = parts\n",
    "        sentiwords[key] = float(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "074e4ddf-ebff-4d51-9299-374d1424ceef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-05 18:40:58 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json: 370kB [00:00, 10.0MB/s]\n",
      "2024-02-05 18:40:58 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-02-05 18:40:59 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2024-02-05 18:40:59 INFO: Using device: cpu\n",
      "2024-02-05 18:40:59 INFO: Loading: tokenize\n",
      "2024-02-05 18:40:59 INFO: Loading: mwt\n",
      "2024-02-05 18:40:59 INFO: Loading: pos\n",
      "2024-02-05 18:40:59 INFO: Loading: lemma\n",
      "2024-02-05 18:40:59 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize, pos, lemma', tokenize_no_ssplit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a03421fb-06f9-4b10-ba96-3c9c7c7e7519",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_mapping = { 'NOUN': 'n',\n",
    "                'VERB': 'v',\n",
    "                'ADJ': 'a',\n",
    "                'ADV': 'r' }\n",
    "\n",
    "def classify_sentiment(sentence, threshold):\n",
    "    doc = nlp(sentence)\n",
    "    sentiment_score = 0\n",
    "    words_count = 0\n",
    "\n",
    "    pred = []\n",
    "    for sent in doc.sentences:\n",
    "        sent_scores = []\n",
    "        for word in sent.words:\n",
    "            pos = pos_mapping.get(word.upos)\n",
    "            if pos:\n",
    "                key = f\"{word.lemma}#{pos}\"\n",
    "                if key in sentiwords:\n",
    "                    sentiment_score += sentiwords[key]\n",
    "                    words_count += 1\n",
    "\n",
    "        if words_count > 0:\n",
    "            average_sentiment = sentiment_score / words_count\n",
    "            if average_sentiment > 0: pred.append(1)\n",
    "            else: pred.append(0)\n",
    "        else:\n",
    "            pred.append(0)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06121554-09a7-47c2-a7c6-32f7ab5e12e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "best_threshold = 0.5\n",
    "best_acc = 0.0\n",
    "\n",
    "for threshold in np.arange(0, 1, 0.1):\n",
    "    val_pred = classify_sentiment(x_val_t.tolist(), threshold)\n",
    "    acc = accuracy_score(y_val.tolist(), val_pred)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_threshold = threshold\n",
    "print(best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca9ff1cd-445c-4654-a258-e0f4409af33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.505\n"
     ]
    }
   ],
   "source": [
    "test_pred = classify_sentiment(x_test_t.tolist(), best_threshold)\n",
    "acc = accuracy_score(y_test, test_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dda5faa8-7e70-4674-9489-aed17d0208b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I wanna die from happiness\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55086cf-0dff-471f-80af-672ae1475547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
