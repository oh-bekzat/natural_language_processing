{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.model_selection as skm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('yelp_labelled.txt', header=None, sep='\\t')\n",
    "X_text = data[0]\n",
    "labels = data[1]\n",
    "X_text_t0, X_text_test, y_train_t0, y_test = skm.train_test_split(X_text, labels, test_size=1/5, stratify=labels, random_state=1234)\n",
    "X_text_train, X_text_val, y_train, y_val = skm.train_test_split(X_text_t0, y_train_t0, test_size=1/8, stratify=y_train_t0, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "ft = fasttext.load_model('cc.en.300.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func to vecorize text using fasttext\n",
    "def vectorize_texts(texts):\n",
    "    return [ft.get_sentence_vector(text) for text in texts]\n",
    "\n",
    "X_train = vectorize_texts(X_text_train)\n",
    "X_val = vectorize_texts(X_text_val)\n",
    "X_test = vectorize_texts(X_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for config {'C': 0.1, 'penalty': 'l2'}: 0.64\n",
      "Accuracy for config {'C': 1, 'penalty': 'l2'}: 0.72\n",
      "Accuracy for config {'C': 10, 'penalty': 'l2'}: 0.8\n",
      "Accuracy for config {'C': 0.1, 'penalty': 'l1'}: 0.5\n",
      "Accuracy for config {'C': 1, 'penalty': 'l1'}: 0.67\n",
      "Accuracy for config {'C': 10, 'penalty': 'l1'}: 0.8\n",
      "\n",
      "Best config: {'config': {'C': 10, 'penalty': 'l2'}, 'accuracy': 0.8}\n",
      "\n",
      "Accuracy on test set: 0.805\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "configuraions = [\n",
    "    {'C': 0.1, 'penalty': 'l2'},\n",
    "    {'C': 1, 'penalty': 'l2'},\n",
    "    {'C': 10, 'penalty': 'l2'},\n",
    "    {'C': 0.1, 'penalty': 'l1'},\n",
    "    {'C': 1, 'penalty': 'l1'},\n",
    "    {'C': 10, 'penalty': 'l1'}\n",
    "]\n",
    "\n",
    "best_config = None\n",
    "\n",
    "for config in configuraions:\n",
    "    lr = LogisticRegression(C=config['C'], penalty=config['penalty'], max_iter=10000, solver='liblinear')\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    print(f'Accuracy for config {config}: {accuracy}')\n",
    "    \n",
    "    if best_config is None or accuracy > best_config['accuracy']:\n",
    "        best_config = {'config': config, 'accuracy': accuracy}\n",
    "\n",
    "\n",
    "print(f'\\nBest config: {best_config}\\n')\n",
    "# testing on test set\n",
    "\n",
    "lr = LogisticRegression(C=best_config['config']['C'], penalty=best_config['config']['penalty'], max_iter=10000, solver='liblinear')\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy on test set: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for config {'C': 0.1, 'penalty': 'l2'}: 0.72\n",
      "Accuracy for config {'C': 1, 'penalty': 'l2'}: 0.77\n",
      "Accuracy for config {'C': 10, 'penalty': 'l2'}: 0.77\n",
      "Accuracy for config {'C': 0.1, 'penalty': 'l1'}: 0.5\n",
      "Accuracy for config {'C': 1, 'penalty': 'l1'}: 0.57\n",
      "Accuracy for config {'C': 10, 'penalty': 'l1'}: 0.74\n",
      "\n",
      "Best config: {'config': {'C': 10, 'penalty': 'l2'}, 'accuracy': 0.8}\n",
      "\n",
      "Accuracy on test set: 0.805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI_Dev_Vault\\Anaconda\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_text_train)\n",
    "X_val = vectorizer.transform(X_text_val)\n",
    "X_test = vectorizer.transform(X_text_test)\n",
    "\n",
    "for config in configuraions:\n",
    "    lr = LogisticRegression(C=config['C'], penalty=config['penalty'], max_iter=10000, solver='liblinear')\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    print(f'Accuracy for config {config}: {accuracy}')\n",
    "    \n",
    "    if best_config is None or accuracy > best_config['accuracy']:\n",
    "        best_config = {'config': config, 'accuracy': accuracy}\n",
    "        \n",
    "        \n",
    "print(f'\\nBest config: {best_config}\\n')\n",
    "# testing on test set\n",
    "\n",
    "lr = LogisticRegression(C=best_config['config']['C'], penalty=best_config['config']['penalty'], max_iter=1000, solver='liblinear')\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy on test set: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('yelp_labelled.txt', header=None, sep='\\t')\n",
    "X_text = data[0]\n",
    "y = data[1]\n",
    "X_text_t0, X_text_test, y_train_t0, y_test = skm.train_test_split(X_text, y, test_size=0.2, stratify=y, random_state=1234)\n",
    "X_text_train, X_text_val, y_train, y_val = skm.train_test_split(X_text_t0, y_train_t0, test_size=1/8, stratify=y_train_t0, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tunning_lr(x_train, y_train, x_val, y_val, x_test, y_test, configuraions):\n",
    "    best_config = None\n",
    "    for config in configuraions:\n",
    "        lr = LogisticRegression(C=config['C'], penalty=config['penalty'], max_iter=10000, solver='liblinear')\n",
    "        lr.fit(x_train, y_train)\n",
    "        y_pred = lr.predict(x_val)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        print(f'Accuracy for config {config}: {accuracy}')\n",
    "        if best_config is None or accuracy > best_config['accuracy']:\n",
    "            best_config = {'config': config, 'accuracy': accuracy}\n",
    "    print(f'\\nBest config: {best_config}\\n')\n",
    "    lr = LogisticRegression(C=best_config['config']['C'], penalty=best_config['config']['penalty'], max_iter=10000, solver='liblinear')\n",
    "    lr.fit(x_train, y_train)\n",
    "    y_pred = lr.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy on test set: {accuracy}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for config {'C': 0.1, 'penalty': 'l2'}: 0.89\n",
      "Accuracy for config {'C': 1, 'penalty': 'l2'}: 0.91\n",
      "Accuracy for config {'C': 10, 'penalty': 'l2'}: 0.88\n",
      "Accuracy for config {'C': 0.1, 'penalty': 'l1'}: 0.5\n",
      "Accuracy for config {'C': 1, 'penalty': 'l1'}: 0.92\n",
      "Accuracy for config {'C': 10, 'penalty': 'l1'}: 0.85\n",
      "\n",
      "Best config: {'config': {'C': 1, 'penalty': 'l1'}, 'accuracy': 0.92}\n",
      "\n",
      "Accuracy on test set: 0.92\n",
      "\n",
      "Accuracy for config {'C': 0.1, 'penalty': 'l2'}: 0.93\n",
      "Accuracy for config {'C': 1, 'penalty': 'l2'}: 0.92\n",
      "Accuracy for config {'C': 10, 'penalty': 'l2'}: 0.92\n",
      "Accuracy for config {'C': 0.1, 'penalty': 'l1'}: 0.5\n",
      "Accuracy for config {'C': 1, 'penalty': 'l1'}: 0.92\n",
      "Accuracy for config {'C': 10, 'penalty': 'l1'}: 0.93\n",
      "\n",
      "Best config: {'config': {'C': 0.1, 'penalty': 'l2'}, 'accuracy': 0.93}\n",
      "\n",
      "Accuracy on test set: 0.93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model1 = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model2 = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "X_train_s = X_text_train.tolist()\n",
    "X_val_s = X_text_val.tolist()\n",
    "\n",
    "X_train1 = model1.encode(X_train_s)\n",
    "X_val1 = model1.encode(X_val_s)\n",
    "\n",
    "X_train2 = model2.encode(X_train_s)\n",
    "X_val2 = model2.encode(X_val_s)\n",
    "\n",
    "tunning_lr(X_train1, y_train, X_val1, y_val, X_val1, y_val, configuraions)\n",
    "tunning_lr(X_train2, y_train, X_val2, y_val, X_val2, y_val, configuraions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949f06f3ec7b46bb99930a95eb29b175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-03 21:52:06 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-03-03 21:52:07 INFO: File exists: C:\\Users\\Dastan\\stanza_resources\\en\\default.zip\n",
      "2024-03-03 21:52:25 INFO: Finished downloading models and saved to C:\\Users\\Dastan\\stanza_resources.\n",
      "2024-03-03 21:52:25 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097c9c3313334c308c2c11725fcc90b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-03 21:52:34 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-03-03 21:52:35 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| mwt       | combined |\n",
      "| sentiment | sstplus  |\n",
      "========================\n",
      "\n",
      "2024-03-03 21:52:35 INFO: Using device: cpu\n",
      "2024-03-03 21:52:35 INFO: Loading: tokenize\n",
      "2024-03-03 21:52:35 INFO: Loading: mwt\n",
      "2024-03-03 21:52:35 INFO: Loading: sentiment\n",
      "2024-03-03 21:52:36 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with class 0 as neutral: 0.98\n",
      "Accuracy with class 1 as neutral: 0.95\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment')\n",
    "\n",
    "docs = [nlp(sentence) for sentence in X_val_s]\n",
    "\n",
    "# Using the validation set, decide which class to assign to the texts classified as neutral\n",
    "classes = [0, 1] # 0: negative, 1: positive\n",
    "\n",
    "# stanza gives 0 for negative, 1 for neutral and 2 for positive, so we have to change the neutral class and change postive class to 1\n",
    "for class_ in classes:\n",
    "    y_pred = [doc.sentences[0].sentiment for doc in docs] # 0: negative, 1: neutral, 2: positive\n",
    "    #changing neutral classes to class_\n",
    "    y_pred = [class_ if sentiment == 1 else sentiment for sentiment in y_pred]\n",
    "    #changing positive classes to 1\n",
    "    y_pred = [1 if sentiment == 2 else sentiment for sentiment in y_pred]\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    print(f'\\nAccuracy with class {class_} as neutral: {accuracy}\\n')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting neutral texts to negative resulted in higher accuracy. Now we apply to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = [nlp(sentence) for sentence in X_text_test.tolist()]\n",
    "\n",
    "y_pred = [doc.sentences[0].sentiment for doc in docs]\n",
    "y_pred = [0 if sentiment == 0 else 1 for sentiment in y_pred]\n",
    "y_pred = [1 if sentiment == 2 else sentiment for sentiment in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy on test set: {accuracy}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
