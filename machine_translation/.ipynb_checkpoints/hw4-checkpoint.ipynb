{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4026148e-c243-4721-9d6f-3c005adf5bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch as torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e72da816-03a9-4854-b173-f0ad972ad3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|█████████████████████| 25.6M/25.6M [00:38<00:00, 659kB/s]\n",
      "Generating dev split: 997 examples [00:00, 55211.53 examples/s]\n",
      "Generating devtest split: 1012 examples [00:00, 66917.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('facebook/flores', 'rus_Cyrl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a1050e9-399d-4d1f-98ec-5504f31b13cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin: 100%|███████████████████| 2.46G/2.46G [02:36<00:00, 15.8MB/s]\n",
      "generation_config.json: 100%|███████████████████| 189/189 [00:00<00:00, 756kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5afbd7c-2317-4ea3-973b-6f2c00e992d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1, 'URL': 'https://en.wikinews.org/wiki/Toronto_team-led_research_on_Type_1_Diabetes_%27groundbreaking%27', 'domain': 'wikinews', 'topic': 'disease, research, canada', 'has_image': 0, 'has_hyperlink': 0, 'sentence': '\"Теперь у нас есть четырёхмесячные мыши, у которых больше нет диабета\", — добавил он.'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['devtest'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3308347c-37a7-400e-9b58-d1695b034ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = { \n",
    "    'And a similar loss moved Brandon Maxwell, who channeled grief and its emotional stages into a collection that drew from nature and the American Southwest.',\n",
    "    'Overall, the family found their European ski holiday a far better value than in the United States.',\n",
    "    'Dubai has 18 restaurants in the top 50, while Amman, Cairo and Tel Aviv tie for second place with five each, and Marrakech and Beirut take joint third with three restaurants apiece.',\n",
    "    'That’s because they are exposed to the elements much more often, and that environmental damage affects the wheels.',\n",
    "    'At the same time, OpenAI said Sora is still a work in progress with clear “weaknesses,” particularly when it comes to spatial details of a prompt – mixing up left and right – and cause and effect.',\n",
    "    '',\n",
    "    '',\n",
    "    ''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75ee603d-767a-4fb0-99ec-7623e066954c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Step 3: Tokenize the input\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Encode the text and add the target language token at the beginning.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 13\u001b[0m encoded_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcat([torch\u001b[38;5;241m.\u001b[39mtensor([[tgt_lang_token]] \u001b[38;5;241m*\u001b[39m encoded_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), encoded_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]], \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Step 4: Translate the text\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Forward pass, get translated token IDs\u001b[39;00m\n\u001b[1;32m     17\u001b[0m translated_tokens \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoded_input)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world! This is a test translation.\"\n",
    "\n",
    "# You need to specify the source and target languages. This is usually done through\n",
    "# the tokenizer's `src_lang` and `tgt_lang` special tokens. For NLLB models, language\n",
    "# codes are typically ISO 639-1 or ISO 639-3 codes, but refer to the model's documentation\n",
    "# for exact codes. For English to French translation:\n",
    "tokenizer.src_lang = \"en\"\n",
    "tgt_lang_token = tokenizer.lang_code_to_id[\"rus_Cyrl\"]\n",
    "\n",
    "# Step 3: Tokenize the input\n",
    "# Encode the text and add the target language token at the beginning.\n",
    "encoded_input = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "encoded_input[\"input_ids\"] = torch.cat([torch.tensor([[tgt_lang_token]] * encoded_input[\"input_ids\"].shape[0]), encoded_input[\"input_ids\"]], 1)\n",
    "\n",
    "# Step 4: Translate the text\n",
    "# Forward pass, get translated token IDs\n",
    "translated_tokens = model.generate(**encoded_input)\n",
    "\n",
    "# Step 5: Decode the token IDs to text\n",
    "translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734f2d47-1b7a-4acf-a4d9-dd4e2aebfb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
